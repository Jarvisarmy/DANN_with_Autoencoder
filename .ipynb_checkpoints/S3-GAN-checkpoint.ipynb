{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79338d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from zipfile import ZipFile\n",
    "import skimage.io\n",
    "from PIL import Image\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "import random\n",
    "import umap\n",
    "from itertools import product\n",
    "\n",
    "from mnist_generator import get_mnist_loaders\n",
    "from mnistm_generator import get_mnistm_loaders\n",
    "from DANN import *\n",
    "from DA import *\n",
    "from test import *\n",
    "from train import *\n",
    "from visualize import *\n",
    "from util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e836aeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "496399e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train_loader,mnist_eval_loader, mnist_test_loader = get_mnist_loaders(batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc6b60a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnistm_train_loader, mnistm_eval_loader,mnistm_test_loader = get_mnistm_loaders(batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0d340c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class S3GAN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(S3GAN, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            # 3x28x28\n",
    "            nn.Conv2d(3,8, 3),\n",
    "            nn.InstanceNorm2d(8),\n",
    "            nn.ReLU(),\n",
    "            # 8x26x26\n",
    "            nn.Conv2d(8,16, 3, stride=2,padding=1)\n",
    "            nn.InstanceNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            # 16x13x13\n",
    "            nn.Conv2d(16,32, 3, stride=2,padding=1),\n",
    "            nn.InstanceNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            # 32x8x8\n",
    "            nn.Conv2d(32,64,3, stride=2,padding=1),\n",
    "            nn.InstanceNorm2d(64),\n",
    "            nn.ReLU()\n",
    "            # 64x4x4\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            # 64x4x4\n",
    "            nn.ConvTranspose2d(64, 32, 3,stride=2,padding=1),\n",
    "            nn.InstanceNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            # 32x7x7\n",
    "            nn.ConvTranspose2d(32,16,3,stride=2,padding=1,output_padding=1),\n",
    "            nn.InstanceNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            # 16x14x14\n",
    "            nn.ConvTranspose2d(16, 3,padding=1,output_padding=1)\n",
    "            nn.InstanceNorm2d(3),\n",
    "            nn.ReLU(),\n",
    "            # 3x28x28\n",
    "        )\n",
    "    def forward(self, A,B):\n",
    "        latentA = self.encoder(A)\n",
    "        latentB = self.encoder(B)\n",
    "        rec_A = self.decoder(latentA)\n",
    "        rec_B = self.decoder(latentB)\n",
    "        \n",
    "        latentA.detach()\n",
    "        latentB.detach()\n",
    "        \n",
    "        style = latentA[:,0:32,:,:]\n",
    "        content = latentB[:,32:54,:,:]\n",
    "        \n",
    "        mixed_latent = torch.cat([style,content],dim=1)\n",
    "        mixed_image = torch.decoder(mixed_latent)\n",
    "        \n",
    "        latent_rec_A = self.encoder(rec_A)\n",
    "        latent_red_B = self.encoder(rec_B)\n",
    "        latent_rec_A.detach()\n",
    "        latent_rec_B.detach()\n",
    "        \n",
    "        rec_style = latent_rec_A[:,0:32,:,:]\n",
    "        rec_content = latent_rec_B[:,32:64,:,:]\n",
    "        \n",
    "        mixed_rec_latent = torch.cat([rec_style,rec_content],dim=1)\n",
    "        mixed_red_image = self.decoder(mixed_rec_latent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28f3c81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator_loss = torch.nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "662eb253",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gramian_matrix(x):\n",
    "    _, d, h, w = x.size()\n",
    "    x = x.view(d, h*w)\n",
    "    gram = torch.mm(x,x.t())\n",
    "    return gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8654564d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conceptual_style_loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Conceptual_style_loss, self).__init__()\n",
    "        \n",
    "    def forward(self, x1, x2):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f01e86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
